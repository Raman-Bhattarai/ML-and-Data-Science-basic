{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f866a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example small training (for demo, not full CIFAR-10 yet)\n",
    "\n",
    "# Load CIFAR-10 with keras for simplicity (30MB)\n",
    "from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58857ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        # Xavier init\n",
    "        limit = np.sqrt(6 / (in_channels * kernel_size * kernel_size + out_channels))\n",
    "        self.W = np.random.uniform(-limit, limit, \n",
    "                                   (out_channels, in_channels, kernel_size, kernel_size))\n",
    "        self.b = np.zeros((out_channels, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        N, C, H, W = x.shape\n",
    "        k, s, p = self.kernel_size, self.stride, self.padding\n",
    "        \n",
    "        H_out = (H - k + 2*p)//s + 1\n",
    "        W_out = (W - k + 2*p)//s + 1\n",
    "        self.out = np.zeros((N, self.out_channels, H_out, W_out))\n",
    "        \n",
    "        x_padded = np.pad(x, ((0,0),(0,0),(p,p),(p,p)), mode=\"constant\")\n",
    "        \n",
    "        for n in range(N):\n",
    "            for c_out in range(self.out_channels):\n",
    "                for i in range(H_out):\n",
    "                    for j in range(W_out):\n",
    "                        h_start, w_start = i*s, j*s\n",
    "                        patch = x_padded[n, :, h_start:h_start+k, w_start:w_start+k]\n",
    "                        self.out[n, c_out, i, j] = np.sum(patch * self.W[c_out]) + self.b[c_out]\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, d_out, lr=0.001):\n",
    "        N, C, H, W = self.x.shape\n",
    "        k, s, p = self.kernel_size, self.stride, self.padding\n",
    "        H_out, W_out = d_out.shape[2], d_out.shape[3]\n",
    "        \n",
    "        dx = np.zeros_like(self.x)\n",
    "        dW = np.zeros_like(self.W)\n",
    "        db = np.zeros_like(self.b)\n",
    "        \n",
    "        x_padded = np.pad(self.x, ((0,0),(0,0),(p,p),(p,p)), mode=\"constant\")\n",
    "        dx_padded = np.pad(dx, ((0,0),(0,0),(p,p),(p,p)), mode=\"constant\")\n",
    "        \n",
    "        for n in range(N):\n",
    "            for c_out in range(self.out_channels):\n",
    "                for i in range(H_out):\n",
    "                    for j in range(W_out):\n",
    "                        h_start, w_start = i*s, j*s\n",
    "                        patch = x_padded[n, :, h_start:h_start+k, w_start:w_start+k]\n",
    "                        \n",
    "                        dW[c_out] += d_out[n, c_out, i, j] * patch\n",
    "                        db[c_out] += d_out[n, c_out, i, j]\n",
    "                        dx_padded[n, :, h_start:h_start+k, w_start:w_start+k] += d_out[n, c_out, i, j] * self.W[c_out]\n",
    "        \n",
    "        dx = dx_padded[:, :, p:-p, p:-p] if p > 0 else dx_padded\n",
    "        # update\n",
    "        self.W -= lr * dW\n",
    "        self.b -= lr * db\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bd2b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        return d_out * (self.x > 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c69512",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D:\n",
    "    def __init__(self, size=2, stride=2):\n",
    "        self.size = size\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        N, C, H, W = x.shape\n",
    "        s = self.stride\n",
    "        H_out, W_out = H//s, W//s\n",
    "        self.out = np.zeros((N, C, H_out, W_out))\n",
    "        self.argmax = np.zeros_like(x, dtype=bool)\n",
    "        \n",
    "        for n in range(N):\n",
    "            for c in range(C):\n",
    "                for i in range(H_out):\n",
    "                    for j in range(W_out):\n",
    "                        h_start, w_start = i*s, j*s\n",
    "                        patch = x[n, c, h_start:h_start+s, w_start:w_start+s]\n",
    "                        max_val = np.max(patch)\n",
    "                        self.out[n, c, i, j] = max_val\n",
    "                        self.argmax[n, c, h_start:h_start+s, w_start:w_start+s] = (patch == max_val)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        dx = np.zeros_like(self.x)\n",
    "        N, C, H_out, W_out = d_out.shape\n",
    "        s = self.stride\n",
    "        \n",
    "        for n in range(N):\n",
    "            for c in range(C):\n",
    "                for i in range(H_out):\n",
    "                    for j in range(W_out):\n",
    "                        h_start, w_start = i*s, j*s\n",
    "                        dx[n, c, h_start:h_start+s, w_start:w_start+s] += d_out[n, c, i, j] * self.argmax[n, c, h_start:h_start+s, w_start:w_start+s]\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398b97dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected:\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        limit = np.sqrt(6 / (in_dim + out_dim))\n",
    "        self.W = np.random.uniform(-limit, limit, (in_dim, out_dim))\n",
    "        self.b = np.zeros((1, out_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x.reshape(x.shape[0], -1)\n",
    "        return self.x @ self.W + self.b\n",
    "    \n",
    "    def backward(self, d_out, lr=0.001):\n",
    "        dW = self.x.T @ d_out\n",
    "        db = np.sum(d_out, axis=0, keepdims=True)\n",
    "        dx = d_out @ self.W.T\n",
    "        self.W -= lr * dW\n",
    "        self.b -= lr * db\n",
    "        return dx.reshape((-1,) + self.x.shape[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x = x - np.max(x, axis=1, keepdims=True)  # stability\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy(pred, y):\n",
    "    N = pred.shape[0]\n",
    "    log_likelihood = -np.log(pred[np.arange(N), y] + 1e-9)\n",
    "    return np.sum(log_likelihood) / N\n",
    "\n",
    "def softmax_backward(pred, y):\n",
    "    grad = pred.copy()\n",
    "    grad[np.arange(len(y)), y] -= 1\n",
    "    return grad / len(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d87ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN:\n",
    "    def __init__(self):\n",
    "        self.conv1 = Conv2D(3, 8, kernel_size=3, padding=1)\n",
    "        self.relu1 = ReLU()\n",
    "        self.pool1 = MaxPool2D(2,2)\n",
    "        \n",
    "        self.conv2 = Conv2D(8, 16, kernel_size=3, padding=1)\n",
    "        self.relu2 = ReLU()\n",
    "        self.pool2 = MaxPool2D(2,2)\n",
    "        \n",
    "        self.fc = FullyConnected(16*8*8, 10)  # CIFAR-10, input 32x32 â†’ pooled to 8x8\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1.forward(x)\n",
    "        out = self.relu1.forward(out)\n",
    "        out = self.pool1.forward(out)\n",
    "        \n",
    "        out = self.conv2.forward(out)\n",
    "        out = self.relu2.forward(out)\n",
    "        out = self.pool2.forward(out)\n",
    "        \n",
    "        out = self.fc.forward(out)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, d_out, lr=0.001):\n",
    "        d_out = self.fc.backward(d_out, lr)\n",
    "        \n",
    "        d_out = self.pool2.backward(d_out)\n",
    "        d_out = self.relu2.backward(d_out)\n",
    "        d_out = self.conv2.backward(d_out, lr)\n",
    "        \n",
    "        d_out = self.pool1.backward(d_out)\n",
    "        d_out = self.relu1.backward(d_out)\n",
    "        d_out = self.conv1.backward(d_out, lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278d95a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "y_train, y_test = y_train.flatten(), y_test.flatten()\n",
    "\n",
    "# Normalize\n",
    "x_train = x_train.astype(np.float32) / 255.0\n",
    "x_test  = x_test.astype(np.float32) / 255.0\n",
    "\n",
    "# NCHW format\n",
    "x_train = np.transpose(x_train, (0,3,1,2))\n",
    "x_test  = np.transpose(x_test, (0,3,1,2))\n",
    "\n",
    "model = SimpleCNN()\n",
    "\n",
    "EPOCHS = 2\n",
    "BATCH = 32\n",
    "LR = 0.001\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    idx = np.random.permutation(len(x_train))\n",
    "    x_train, y_train = x_train[idx], y_train[idx]\n",
    "    \n",
    "    for i in range(0, len(x_train), BATCH):\n",
    "        xb, yb = x_train[i:i+BATCH], y_train[i:i+BATCH]\n",
    "        \n",
    "        # forward\n",
    "        logits = model.forward(xb)\n",
    "        probs = softmax(logits)\n",
    "        loss = cross_entropy(probs, yb)\n",
    "        \n",
    "        # backward\n",
    "        d_out = softmax_backward(probs, yb)\n",
    "        model.backward(d_out, lr=LR)\n",
    "        \n",
    "    # quick eval\n",
    "    test_logits = model.forward(x_test[:500])  # subset for speed\n",
    "    test_preds = np.argmax(softmax(test_logits), axis=1)\n",
    "    acc = np.mean(test_preds == y_test[:500])\n",
    "    print(f\"Epoch {epoch+1}, Loss={loss:.4f}, Test Acc={acc:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
