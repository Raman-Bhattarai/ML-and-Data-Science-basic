{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "185a4758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74322798",
   "metadata": {},
   "source": [
    "We use MNIST dataset (handwritten digits 0–9). It has 70,000 images, each 28×28 pixels → flattened into 784 input features.\n",
    "\n",
    "Dataset size is about 11 MB — small enough for “from scratch” training.\n",
    "\n",
    "Features (X) are normalized to [0, 1] by dividing by 255, which helps avoid exploding gradients during training.\n",
    "\n",
    "Labels (y) are converted into one-hot encoded vectors.\n",
    "Example: digit 3 → [0,0,0,1,0,0,0,0,0,0].\n",
    "         digit 5 → [0,0,0,0,0,1,0,0,0,0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa5e77ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "314cd9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X / 255.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2e34c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(sparse_output=False, categories=\"auto\")\n",
    "y = enc.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d655ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446a4e6b",
   "metadata": {},
   "source": [
    "Input layer: 784 nodes (one for each pixel).\n",
    "\n",
    "Hidden layer: 128 neurons. This number is a hyperparameter (too few = underfitting, too many = slow training).\n",
    "\n",
    "Output layer: 10 neurons (since MNIST has 10 classes (numbers 0 to 9)).\n",
    "\n",
    "Learning Rate: 0.02 (Controls how big the update step is. If learning rate is too small the update may take too long to reach optimized value of weights or biases. And if learning rate is too large the update may miss the value and never reach the required optimized value.)\n",
    "\n",
    "Epochs: 30 (Each epoch is complete training of dataset 1 time by the learning algorithm. So, the dataset is trained 20 \n",
    "            times with updated values (weight and bias) learned from each epoch to make the model better.)\n",
    "\n",
    "#Parameters initialized:\n",
    "\n",
    "    - Weights (W1, W2): Randomly initialized with Xavier initialization to keep signal variance stable across layers.\n",
    "\n",
    "    - Biases (b1, b2): Initialized to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "544dac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784   # MNIST images 28x28\n",
    "hidden_size = 128\n",
    "output_size = 10\n",
    "lr = 0.03\n",
    "epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae3ab436",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "W1 = np.random.randn(input_size, hidden_size) * np.sqrt(1. / input_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size) * np.sqrt(1. / hidden_size)\n",
    "b2 = np.zeros((1, output_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9b1917",
   "metadata": {},
   "source": [
    "ReLU (Rectified Linear Unit) for hidden layer:\n",
    "\n",
    "    - Formula: f(x) = max(0, x).\n",
    "\n",
    "    - Purpose: introduces non-linearity so the network can learn complex mappings.\n",
    "\n",
    "    - Derivative: 1 if x>0, otherwise 0. Used during backprop.\n",
    "\n",
    "Softmax for output layer:\n",
    "\n",
    "    - Converts raw scores into probabilities: softmax(z_i) = exp(z_i) / Σ exp(z_j)\n",
    "\n",
    "    - Ensures outputs sum to 1, making them interpretable as probabilities.\n",
    "\n",
    "Cross-Entropy Loss:\n",
    "\n",
    "    - Measures the difference between predicted probability distribution and actual labels.\n",
    "\n",
    "    - Formula: L = -(1/m) Σ y_true * log(y_pred)\n",
    "\n",
    "    - Penalizes the model heavily when it assigns low probability to the correct class.\n",
    "\n",
    "\n",
    "Lower loss = better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a421971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    return -np.sum(y_true * np.log(y_pred + 1e-8)) / m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faea6ef1",
   "metadata": {},
   "source": [
    "# Forward Propagation:\n",
    "\n",
    "    Compute weighted sum at hidden layer: Z1 = X·W1 + b1\n",
    "\n",
    "    Apply activation (ReLU): A1 = ReLU(Z1)\n",
    "\n",
    "    Compute weighted sum at output layer: Z2 = A1·W2 + b2\n",
    "\n",
    "    Apply softmax: A2 = softmax(Z2)\n",
    "    → gives probability distribution over 10 classes.\n",
    "\n",
    "    This forward pass predicts outputs based on current weights.\n",
    "\n",
    "# Backward Propagation:\n",
    "\n",
    "    Output layer error: dZ2 = A2 - y_true\n",
    "    (difference between predicted probabilities and true labels).\n",
    "\n",
    "    Gradients for output weights/bias:  dW2 = (A1ᵀ·dZ2)/m\n",
    "                                        db2 = sum(dZ2)/m\n",
    "\n",
    "    Hidden layer error: dA1 = dZ2·W2ᵀ\n",
    "                        dZ1 = dA1 * ReLU'(Z1)\n",
    "    (applying derivative of ReLU).\n",
    "\n",
    "    Gradients for hidden weights/bias:  dW1 = (Xᵀ·dZ1)/m\n",
    "                                        db1 = sum(dZ1)/m\n",
    "\n",
    "    These gradients tell us how much each weight/bias contributed to the error.\n",
    "\n",
    "# Parameter Update:\n",
    "\n",
    "    Gradient Descent is used:   W = W - lr * dW\n",
    "                                b = b - lr * db\n",
    "\n",
    "    lr = learning rate (0.01). Controls how big the update step is.\n",
    "\n",
    "    Repeat across epochs (20 in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8266f2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Loss: 2.3386\n",
      "Epoch 3/40, Loss: 2.3041\n",
      "Epoch 5/40, Loss: 2.2720\n",
      "Epoch 7/40, Loss: 2.2417\n",
      "Epoch 9/40, Loss: 2.2128\n",
      "Epoch 11/40, Loss: 2.1847\n",
      "Epoch 13/40, Loss: 2.1573\n",
      "Epoch 15/40, Loss: 2.1302\n",
      "Epoch 17/40, Loss: 2.1032\n",
      "Epoch 19/40, Loss: 2.0764\n",
      "Epoch 21/40, Loss: 2.0496\n",
      "Epoch 23/40, Loss: 2.0227\n",
      "Epoch 25/40, Loss: 1.9957\n",
      "Epoch 27/40, Loss: 1.9686\n",
      "Epoch 29/40, Loss: 1.9415\n",
      "Epoch 31/40, Loss: 1.9142\n",
      "Epoch 33/40, Loss: 1.8869\n",
      "Epoch 35/40, Loss: 1.8596\n",
      "Epoch 37/40, Loss: 1.8322\n",
      "Epoch 39/40, Loss: 1.8049\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    Z1 = np.dot(X_train, W1) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = softmax(Z2)\n",
    "\n",
    "    # Loss\n",
    "    loss = cross_entropy(y_train, A2)\n",
    "\n",
    "    # Backpropagation\n",
    "    m = X_train.shape[0]\n",
    "    dZ2 = A2 - y_train\n",
    "    dW2 = np.dot(A1.T, dZ2) / m\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "\n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    dZ1 = dA1 * relu_derivative(Z1)\n",
    "    dW1 = np.dot(X_train.T, dZ1) / m\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "\n",
    "    # Update parameters\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13f1c157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 66.54%\n"
     ]
    }
   ],
   "source": [
    "Z1 = np.dot(X_test, W1) + b1\n",
    "A1 = relu(Z1)\n",
    "Z2 = np.dot(A1, W2) + b2\n",
    "A2 = softmax(Z2)\n",
    "\n",
    "preds = np.argmax(A2, axis=1)\n",
    "labels = np.argmax(y_test, axis=1)\n",
    "acc = np.mean(preds == labels)\n",
    "\n",
    "print(f\"Test Accuracy: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef0dde4",
   "metadata": {},
   "source": [
    "At 20 epoch and 0.01 learning rate the accuracy was obtained 23.93%. \n",
    "\n",
    "Also the accuracy at 30 epoch and 0.02 learning rate obtained was 48.11%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
